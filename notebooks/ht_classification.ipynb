{"cells":[{"cell_type":"markdown","metadata":{"id":"zaul3s35MUXF"},"source":["This Notebook can be viewed at the following link: https://colab.research.google.com/drive/15IUTc-BUjwehb2FdDJq44-64zN66KKy9?usp=sharing\n","\n","Authors:\n","\n","* Stefano Ribes, ribes.stefano@gmail.com | stefano.ribes@gaisler.com\n","* Alessandro Palumbo, alessandro.palumbo@uniroma2.it\n","\n","# HTH Detection and Classification"]},{"cell_type":"markdown","metadata":{"id":"wLELn_ziYYEj"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1664733790630,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"vVbTrGnIMQQr","outputId":"05996d5f-073d-4f65-a9be-05e8bd98f071"},"outputs":[],"source":["import os\n","\n","data_dir = os.path.join(os.path.abspath(''), '..', 'data')\n","models_dir = os.path.join(data_dir, 'models')\n","fig_dir = os.path.join(data_dir, 'figures')\n","\n","# if not os.path.exists(data_dir):\n","#     data_dir = os.path.join('/', 'tmp')\n","#     !gdown https://drive.google.com/file/d/1gKBc8CyZnC0bYl9WzKEFc4e2BhL4lvU0/view?usp=sharing -O /tmp/triggered.csv\n","#     !gdown https://drive.google.com/file/d/1gKBc8CyZnC0bYl9WzKEFc4e2BhL4lvU0/view?usp=sharing -O /tmp/not_triggered.csv\n","#     !cat /tmp/triggered.csv\n","\n","data_dir"]},{"cell_type":"markdown","metadata":{"id":"_jSg4P7C9qWf"},"source":["## Data Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","triggered_df = pd.read_csv(os.path.join(data_dir, 'raw', 'triggered.csv'))\n","not_triggered_df = pd.read_csv(os.path.join(data_dir, 'raw', 'not_triggered.csv'))\n","\n","triggered_df.describe().round(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["not_triggered_df.describe().round(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["old2new = {\n","    'benchmark': 'benchmark',\n","    '% dyn pow': 'avg_dynamic_power',\n","    'power': 'avg_static_power',\n","    'FF': 'FF',\n","    'LUT': 'LUT',\n","    'timing': 'worst_negative_slack',\n","    'cycles': 'num_cycles',\n","    'instr ret': 'num_instructions_retired',\n","    'LSU': 'datamem_wait_cycles',\n","    'fetch wait': 'fetch_wait_cycles',\n","    'load': 'num_load_instructions',\n","    'store': 'num_store_instructions',\n","    'jump': 'num_jump_instructions',\n","    'cond br': 'num_conditional_branch_instructions',\n","    'tak c bran': 'num_taken_conditional_branch_instructions',\n","    'compr ins': 'num_compressed_instructions',\n","    'mult wait': 'mult_wait_cycles',\n","    'divd wait': 'divd_wait_cycles',\n","    'Temp': 'temperature',\n","    'target': 'ht_type',\n","}\n","# Rename HT classes\n","ht_types = {\n","    0: 'free',\n","    1: 'mitm',\n","    2: 'fetch',\n","    3: 'critical',\n","    4: 'clk_mod',\n","}\n","# Rename dataframes columns\n","triggered_df = triggered_df.rename(columns=old2new).replace({'ht_type': ht_types})\n","not_triggered_df = not_triggered_df.rename(columns=old2new).replace({'ht_type': ht_types})\n","triggered_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax = triggered_df.plot.hist(by='avg_dynamic_power', bins=10, alpha=0.9)\n","ax.plot()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rn30FvgvpMrd"},"source":["### Reading from file"]},{"cell_type":"markdown","metadata":{"id":"lUmoJ2jmxupa"},"source":["Target HTH classes: `golden`, `instr`, `fetch`, `critical` and `clk mod`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"elapsed":2973,"status":"ok","timestamp":1664733795628,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"h3nWUAg3QIIp","outputId":"4daf705a-8fbd-4d80-d67d-ed2e880f2394"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","hth_classes = ['golden', 'instr', 'fetch', 'critical', 'clk_mod']\n","hth_dict = {\n","    0: 'golden',\n","    1: 'instr',\n","    2: 'fetch',\n","    3: 'critical',\n","    4: 'clk_mod'\n","}\n","\n","def get_dataset(triggered_file, not_triggered_file=None, convert_labels=False):\n","    # Read the CSV file of data\n","    data = pd.read_csv(triggered_file).dropna()\n","    data['triggered'] = 1\n","    # Merge two files if required\n","    if not_triggered_file is not None:\n","        data2 = pd.read_csv(not_triggered_file).dropna()\n","        data2['triggered'] = 0\n","        data = pd.concat([data, data2])\n","    # Convert numerical labels to string\n","    if convert_labels:\n","        for hth_id in hth_dict.keys():\n","            data.loc[data['target'] == hth_id, 'target'] = hth_dict[hth_id]\n","    # Shuffle the dataset\n","    return data.sample(frac=1.0, random_state=1)\n","\n","def split_dataset(data_shuffled, train_perc=0.8):\n","    # Splitting into train and test datasets\n","    train_len = int(len(data_shuffled) * train_perc)\n","    data_shuffled_train = data_shuffled.iloc[:train_len, :]\n","    data_shuffled_test = data_shuffled.iloc[train_len:, :]\n","    # Split into input part X and output part Y\n","    # Train\n","    Xtrain = data_shuffled_train.drop(['target', 'triggered'], axis=1)\n","    Ytrain = data_shuffled_train['target'].to_frame()\n","    # Test\n","    Xtest = data_shuffled_test.drop(['target', 'triggered'], axis=1)\n","    Ytest = data_shuffled_test['target'].to_frame()\n","    return Xtrain, Ytrain, Xtest, Ytest\n","\n","triggered = os.path.join(data_dir, 'raw', 'triggered.csv')\n","not_triggered = os.path.join(data_dir, 'raw', 'not_triggered.csv')\n","df = get_dataset(triggered, not_triggered)\n","\n","Xtrain8020, Ytrain8020, Xtest8020, Ytest8020 = split_dataset(df, train_perc=0.8)\n","Xtrain6535, Ytrain6535, Xtest6535, Ytest6535 = split_dataset(df, train_perc=0.65)\n","Xtrain, Ytrain, Xtest, Ytest = split_dataset(df)\n","Xtrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1664733795628,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"QnNHIp5b_sr7","outputId":"0a3ebaaa-7dbe-4cf5-af65-9a2cf8cd6509"},"outputs":[],"source":["len(Xtrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1664733795629,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"0SLH-eUI6FcH","outputId":"ef214c49-8c2c-4e1d-900a-7d344d54d8be"},"outputs":[],"source":["df_triggered = get_dataset(triggered, convert_labels=False)\n","df_not_triggered = get_dataset(not_triggered, convert_labels=False)\n","Xtrain8020_trig, Ytrain8020_trig, Xtest8020_trig, Ytest8020_trig = split_dataset(df_triggered, train_perc=0.8)\n","Xtrain6535_trig, Ytrain6535_trig, Xtest6535_trig, Ytest6535_trig = split_dataset(df_triggered, train_perc=0.65)\n","\n","Xtrain8020_ntrig, Ytrain8020_ntrig, Xtest8020_ntrig, Ytest8020_ntrig = split_dataset(df_not_triggered, train_perc=0.8)\n","Xtrain6535_ntrig, Ytrain6535_ntrig, Xtest6535_ntrig, Ytest6535_ntrig = split_dataset(df_not_triggered, train_perc=0.65)\n","\n","Xtrain8020_trig"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1664733795630,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"4CwpIqhr7UzF","outputId":"1a37ef73-7063-4726-de0a-c2c066e3fca2"},"outputs":[],"source":["print(f'len(Xtrain8020_trig):  {len(Xtrain8020_trig)}')\n","print(f'len(Xtrain6535_trig):  {len(Xtrain6535_trig)}')\n","print(f'len(Xtrain8020_ntrig): {len(Xtrain8020_ntrig)}')\n","print(f'len(Xtrain6535_ntrig): {len(Xtrain6535_ntrig)}')\n","print(f'')\n","print(f'len(Xtest8020_trig):  {len(Xtest8020_trig)}')\n","print(f'len(Xtest6535_trig):  {len(Xtest6535_trig)}')\n","print(f'len(Xtest8020_ntrig): {len(Xtest8020_ntrig)}')\n","print(f'len(Xtest6535_ntrig): {len(Xtest6535_ntrig)}')"]},{"cell_type":"markdown","metadata":{"id":"iWWu2nNrpSkV"},"source":["### Labels Distributions\n","\n","Transform numerical labels (dataframes) into string labels (array of strings) (useful for plotting and inspection)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664733796000,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"wyPWLwLKUbqj","outputId":"33ef0b3c-a773-4b8a-a485-7cf57cb3e4a6"},"outputs":[],"source":["import numpy as np\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","\n","def preprocess_labels(Y, binary_classification=False):\n","    if type(Y) is np.ndarray:\n","        return Y\n","    # Dictionarize labels\n","    dv = DictVectorizer(sparse=False, separator='')\n","    Y_encoded = dv.fit_transform(Y.to_dict('records'))\n","    # Apply labels\n","    le = LabelEncoder()\n","    if binary_classification:\n","        le.fit(['no_hth', 'with_hth'])\n","    else:\n","        le.fit(hth_classes)\n","    return le.inverse_transform(Y_encoded.ravel().astype(int))\n","\n","Ytrain8020 = preprocess_labels(Ytrain8020)\n","Ytest8020 = preprocess_labels(Ytest8020)\n","Ytrain6535 = preprocess_labels(Ytrain6535)\n","Ytest6535 = preprocess_labels(Ytest6535)\n","\n","Ytrain8020_trig = preprocess_labels(Ytrain8020_trig)\n","Ytest8020_trig = preprocess_labels(Ytest8020_trig)\n","Ytrain6535_trig = preprocess_labels(Ytrain6535_trig)\n","Ytest6535_trig = preprocess_labels(Ytest6535_trig)\n","\n","Ytrain8020_ntrig = preprocess_labels(Ytrain8020_ntrig)\n","Ytest8020_ntrig = preprocess_labels(Ytest8020_ntrig)\n","Ytrain6535_ntrig = preprocess_labels(Ytrain6535_ntrig)\n","Ytest6535_ntrig = preprocess_labels(Ytest6535_ntrig)\n","Ytest8020"]},{"cell_type":"markdown","metadata":{"id":"uKvz05t-Ujm4"},"source":["\n","Ideally, since we have 5 classes, each class should appear around 20% in each dataset. The following graphs investigate that."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":1927,"status":"ok","timestamp":1664733800678,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"w7nTLNGqIQM6","outputId":"e3231d40-2cb3-4a2a-f883-33130990781a"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","raw_data = {\n","    'hth_classes': [],\n","    'counts': [],\n","    'categories': []\n","}\n","\n","for hth_type in hth_classes:\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytrain8020 == hth_type) / len(Ytrain8020)]\n","    raw_data['categories'] += ['Ytrain']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytest8020 == hth_type) / len(Ytest8020)]\n","    raw_data['categories'] += ['Ytest']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytrain8020_trig == hth_type) / len(Ytrain8020_trig)]\n","    raw_data['categories'] += ['Ytrain (triggered)']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytest8020_trig == hth_type) / len(Ytest8020_trig)]\n","    raw_data['categories'] += ['Ytest  (triggered)']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytrain8020_ntrig == hth_type) / len(Ytrain8020_ntrig)]\n","    raw_data['categories'] += ['Ytrain (non-triggered)']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytest8020_ntrig == hth_type) / len(Ytest8020_ntrig)]\n","    raw_data['categories'] += ['Ytest  (non-triggered)']\n","\n","ax = sns.barplot(x='hth_classes', y='counts', hue='categories', data=raw_data)\n","\n","ax.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylabel('Class Counts [%]')\n","plt.xlabel('Class Labels')\n","plt.title('HTH classes counts in 80%-20% datasets')\n","# plt.tight_layout()\n","plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":909,"status":"ok","timestamp":1664733801578,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"gpA605c4QvQC","outputId":"b6481ff7-dd73-4b37-a0e5-89ac3742d619"},"outputs":[],"source":["raw_data = {\n","    'hth_classes': [],\n","    'counts': [],\n","    'categories': []\n","}\n","\n","for hth_type in hth_classes:\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytrain6535 == hth_type) / len(Ytrain6535)]\n","    raw_data['categories'] += ['Ytrain']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytest6535 == hth_type) / len(Ytest6535)]\n","    raw_data['categories'] += ['Ytest']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytrain6535_trig == hth_type) / len(Ytrain6535_trig)]\n","    raw_data['categories'] += ['Ytrain (triggered)']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytest6535_trig == hth_type) / len(Ytest6535_trig)]\n","    raw_data['categories'] += ['Ytest  (triggered)']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytrain6535_ntrig == hth_type) / len(Ytrain6535_ntrig)]\n","    raw_data['categories'] += ['Ytrain (non-triggered)']\n","\n","    raw_data['hth_classes'] += [hth_type]\n","    raw_data['counts'] += [sum(Ytest6535_ntrig == hth_type) / len(Ytest6535_ntrig)]\n","    raw_data['categories'] += ['Ytest  (non-triggered)']\n","\n","ax = sns.barplot(x='hth_classes', y='counts', hue='categories', data=raw_data)\n","\n","ax.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylabel('Class Counts [%]')\n","plt.xlabel('Class Labels')\n","plt.title('HTH classes counts in 65%-35% datasets')\n","# plt.tight_layout()\n","plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pFOyQ7hfR1Oc"},"source":["### Dispersion Diagrams"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5893,"status":"ok","timestamp":1664733807467,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"GY-bZyPMR4B3","outputId":"2c666754-a8b7-4a67-84c6-db4ab7aec9ae"},"outputs":[],"source":["triggered = os.path.join(data_dir, 'triggered.csv')\n","not_triggered = os.path.join(data_dir, 'not_triggered.csv')\n","df = get_dataset(triggered, not_triggered, convert_labels=True)\n","df['Samples ID'] = df.index\n","\n","for feature in ['timing', 'LUT', 'FF', 'Temp']:\n","    sns.scatterplot(data=df, x='Samples ID', y=feature, hue='target')\n","    plt.title(f'Dispersion Diagram for feature \"{feature}\"')\n","    plt.grid(which='both', axis='both', alpha=0.7, zorder=1)\n","    plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":17068,"status":"ok","timestamp":1664733824528,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"8Y4jVAefgWzB","outputId":"a76e559c-f13b-40ea-d9b4-16372fecfc28"},"outputs":[],"source":["triggered = os.path.join(data_dir, 'triggered.csv')\n","not_triggered = os.path.join(data_dir, 'not_triggered.csv')\n","df = get_dataset(triggered, not_triggered, convert_labels=True)\n","df['Samples ID'] = df.index\n","\n","for feature in ['cycles', 'instr ret', 'LSU', 'fetch wait', 'load',\n","                        'store', 'jump', 'cond br', 'tak c bran', 'compr ins',\n","                        'mult wait', 'divd wait', 'benchmark', ]:\n","    sns.scatterplot(data=df, x='Samples ID', y=feature, hue='target')\n","    plt.title(f'Dispersion Diagram for feature \"{feature}\"')\n","    plt.grid(which='both', axis='both', alpha=0.7, zorder=1)\n","    plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"elapsed":1844,"status":"ok","timestamp":1664735436230,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"AEqHbSHm3lof","outputId":"3785828c-220b-4b47-aab2-57d9a4ee3867"},"outputs":[],"source":["triggered = os.path.join(data_dir, 'triggered.csv')\n","not_triggered = os.path.join(data_dir, 'not_triggered.csv')\n","df = get_dataset(triggered, not_triggered, convert_labels=True)\n","df['Samples ID'] = df.index\n","\n","df = df[df['cycles'] < 100000]\n","df = df.rename(lambda x: 'Timing [ns]' if x == 'timing' else x, axis='columns')\n","df = df.rename(lambda x: 'Clock Cycles' if x == 'cycles' else x, axis='columns')\n","\n","ax = sns.scatterplot(data=df, y='Timing [ns]', x='Clock Cycles', hue='target')\n","\n","ax.axhline(7.65, ls='--', c='C0')\n","ax.axvline(11500, ls='--', c='C1')\n","\n","# plt.title(f'Dispersion Diagram for feature \"{feature}\"')\n","plt.grid(which='both', axis='both', alpha=0.7, zorder=1)\n","# plt.legend(bbox_to_anchor=(1.04, 0.5), loc='center left')\n","plt.legend(loc='upper right')\n","plt.savefig(os.path.join(fig_dir, f'pf+if_vs_if.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5SJ21DDvgGAB"},"source":["### Box Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2147,"status":"ok","timestamp":1664733826669,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"jv57Nm-UgI2P","outputId":"71c31a1d-0ecc-4e3e-a4a3-401db77850fd"},"outputs":[],"source":["triggered = os.path.join(data_dir, 'triggered.csv')\n","not_triggered = os.path.join(data_dir, 'not_triggered.csv')\n","df = get_dataset(triggered, not_triggered, convert_labels=True)\n","\n","for feature in ['timing', 'LUT', 'FF', 'Temp']:\n","    sns.boxplot(x='target', y=feature, data=df)\n","    plt.title(f'Box plot for feature \"{feature}\"')\n","    plt.grid(which='both', axis='both', alpha=0.7, zorder=1)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HoBawWzciuTW"},"source":["### Features Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":708},"executionInfo":{"elapsed":3373,"status":"ok","timestamp":1664733830391,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"LDJ_V5Ogiw8v","outputId":"8c33c367-e74f-4e97-889c-6b13e91678a6"},"outputs":[],"source":["performance_features = ['cycles', 'instr ret', 'LSU', 'fetch wait', 'load',\n","                        'store', 'jump', 'cond br', 'tak c bran', 'compr ins',\n","                        'mult wait', 'divd wait', 'benchmark', ]\n","synthesis_features = ['FF', 'LUT', 'timing', '% dyn pow', 'power', 'Temp', ]\n","\n","triggered = os.path.join(data_dir, 'triggered.csv')\n","not_triggered = os.path.join(data_dir, 'not_triggered.csv')\n","df = get_dataset(triggered, not_triggered, convert_labels=True)\n","df = df.drop(['triggered'], axis=1)\n","# Order the columns for better distinguishing the two feature sets\n","df = df[performance_features + synthesis_features]\n","\n","corr = df.corr()\n","plt.figure(figsize=(16, 8))\n","sns.heatmap(corr, cmap='vlag', annot=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YLJlovNlHkap"},"source":["### Remove Synthesis Features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454,"status":"ok","timestamp":1664737056461,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"oey5aOd1Hmqe","outputId":"2db8d421-86bc-45b9-9ad8-a4c504fddc5c"},"outputs":[],"source":["df = get_dataset(triggered, not_triggered).drop(synthesis_features, axis=1)\n","\n","Xtrain8020_nosyn, Ytrain8020_nosyn, Xtest8020_nosyn, Ytest8020_nosyn = split_dataset(df, train_perc=0.8)\n","Xtrain6535_nosyn, Ytrain6535_nosyn, Xtest6535_nosyn, Ytest6535_nosyn = split_dataset(df, train_perc=0.65)\n","\n","Ytrain8020_nosyn = preprocess_labels(Ytrain8020_nosyn)\n","Ytest8020_nosyn = preprocess_labels(Ytest8020_nosyn)\n","Ytrain6535_nosyn = preprocess_labels(Ytrain6535_nosyn)\n","Ytest6535_nosyn = preprocess_labels(Ytest6535_nosyn)\n","\n","Ytest8020_nosyn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1664737057775,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"awiUyEVu6cYY","outputId":"01344e54-0495-4e78-e2a8-992eef833c9a"},"outputs":[],"source":["print(f'len(Xtrain8020_nosyn): {len(Xtrain8020_nosyn)}')\n","print(f'len(Xtrain8020_trig):  {len(Xtrain8020_trig)}')\n","print(f'len(Xtrain8020_ntrig): {len(Xtrain8020_ntrig)}')\n","print(f'len(Xtrain6535_nosyn): {len(Xtrain6535_nosyn)}')\n","print(f'len(Xtrain6535_trig):  {len(Xtrain6535_trig)}')\n","print(f'len(Xtrain6535_ntrig): {len(Xtrain6535_ntrig)}')\n","print('')\n","print(f'len(Xtest8020_nosyn): {len(Xtest8020_nosyn)}')\n","print(f'len(Xtest8020_trig):  {len(Xtest8020_trig)}')\n","print(f'len(Xtest8020_ntrig): {len(Xtest8020_ntrig)}')\n","print(f'len(Xtest6535_nosyn): {len(Xtest6535_nosyn)}')\n","print(f'len(Xtest6535_trig):  {len(Xtest6535_trig)}')\n","print(f'len(Xtest6535_ntrig): {len(Xtest6535_ntrig)}')"]},{"cell_type":"markdown","metadata":{"id":"RJ8m4j0bN1aQ"},"source":["### Binary Classification: Balacing the Dataset\n","\n","Generate the datasets for binary classification. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664737058199,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"-GDgfhS6P1WU","outputId":"36cb5d03-4ccc-46a9-f928-1c3d47f4b3c8"},"outputs":[],"source":["df = get_dataset(triggered, not_triggered)\n","df.loc[df['target'] != 0, 'target'] = 1\n","\n","Xtrain8020_bin, Ytrain8020_bin, Xtest8020_bin, Ytest8020_bin = split_dataset(df, train_perc=0.8)\n","Xtrain6535_bin, Ytrain6535_bin, Xtest6535_bin, Ytest6535_bin = split_dataset(df, train_perc=0.65)\n","\n","Ytrain8020_bin = preprocess_labels(Ytrain8020_bin, binary_classification=True)\n","Ytest8020_bin = preprocess_labels(Ytest8020_bin, binary_classification=True)\n","Ytrain6535_bin = preprocess_labels(Ytrain6535_bin, binary_classification=True)\n","Ytest6535_bin = preprocess_labels(Ytest6535_bin, binary_classification=True)\n","\n","Ytest8020_bin"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1664737058997,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"r_FObT_hOQ-5","outputId":"4a496ebd-ea4d-4b8e-f269-62390a628465"},"outputs":[],"source":["df = get_dataset(triggered, not_triggered).drop(synthesis_features, axis=1)\n","df.loc[df['target'] != 0, 'target'] = 1\n","\n","Xtrain8020_bin_nosyn, Ytrain8020_bin_nosyn, Xtest8020_bin_nosyn, Ytest8020_bin_nosyn = split_dataset(df, train_perc=0.8)\n","Xtrain6535_bin_nosyn, Ytrain6535_bin_nosyn, Xtest6535_bin_nosyn, Ytest6535_bin_nosyn = split_dataset(df, train_perc=0.65)\n","\n","Ytrain8020_bin_nosyn = preprocess_labels(Ytrain8020_bin_nosyn, binary_classification=True)\n","Ytest8020_bin_nosyn = preprocess_labels(Ytest8020_bin_nosyn, binary_classification=True)\n","Ytrain6535_bin_nosyn = preprocess_labels(Ytrain6535_bin_nosyn, binary_classification=True)\n","Ytest6535_bin_nosyn = preprocess_labels(Ytest6535_bin_nosyn, binary_classification=True)\n","\n","Ytest8020_bin_nosyn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4jv4X7_N0ht"},"outputs":[],"source":["from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import SMOTE, ADASYN\n","from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1664737059426,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"uLrB83WPRAo0","outputId":"15852ddd-282c-4299-8b09-d369ca856c0b"},"outputs":[],"source":["ros = RandomOverSampler(random_state=0)\n","\n","Xtrain8020_bin_resampled, Ytrain8020_bin_resampled = ros.fit_resample(Xtrain8020_bin, Ytrain8020_bin)\n","Xtrain6535_bin_resampled, Ytrain6535_bin_resampled = ros.fit_resample(Xtrain6535_bin, Ytrain6535_bin)\n","\n","Xtrain8020_bin_resampled_nosyn, Ytrain8020_bin_resampled_nosyn = ros.fit_resample(Xtrain8020_bin_nosyn, Ytrain8020_bin_nosyn)\n","Xtrain6535_bin_resampled_nosyn, Ytrain6535_bin_resampled_nosyn = ros.fit_resample(Xtrain6535_bin_nosyn, Ytrain6535_bin_nosyn)\n","\n","print(sorted(Counter(Ytrain6535_bin_resampled_nosyn).items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1664737059427,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"ltN7xuiySpNN","outputId":"539734a3-8a47-4b6b-995f-adbd7e16b5c7"},"outputs":[],"source":["dvec = DictVectorizer(sparse=False)\n","# SMOTE\n","X = dvec.fit_transform(Xtrain8020_bin.to_dict('records'))\n","Xtrain8020_bin_smote, Ytrain8020_bin_smote = SMOTE().fit_resample(X, Ytrain8020_bin)\n","\n","X = dvec.fit_transform(Xtrain6535_bin.to_dict('records'))\n","Xtrain6535_bin_smote, Ytrain6535_bin_smote = SMOTE().fit_resample(X, Ytrain6535_bin)\n","print(sorted(Counter(Ytrain6535_bin_smote).items()))\n","\n","# No synthesis features\n","X = dvec.fit_transform(Xtrain8020_bin_nosyn.to_dict('records'))\n","Xtrain8020_bin_smote_nosyn, Ytrain8020_bin_smote_nosyn = SMOTE().fit_resample(X, Ytrain8020_bin_nosyn)\n","\n","X = dvec.fit_transform(Xtrain6535_bin_nosyn.to_dict('records'))\n","Xtrain6535_bin_smote_nosyn, Ytrain6535_bin_smote_nosyn = SMOTE().fit_resample(X, Ytrain6535_bin_nosyn)\n","print(sorted(Counter(Ytrain6535_bin_smote_nosyn).items()))\n","\n","# ADASYN\n","X = dvec.fit_transform(Xtrain8020_bin.to_dict('records'))\n","Xtrain8020_bin_adasyn, Ytrain8020_bin_adasyn = ADASYN().fit_resample(X, Ytrain8020_bin)\n","\n","X = dvec.fit_transform(Xtrain6535_bin.to_dict('records'))\n","Xtrain6535_bin_adasyn, Ytrain6535_bin_adasyn = ADASYN().fit_resample(X, Ytrain6535_bin)\n","print(sorted(Counter(Ytrain6535_bin_adasyn).items()))\n","\n","# No synthesis features\n","X = dvec.fit_transform(Xtrain8020_bin_nosyn.to_dict('records'))\n","Xtrain8020_bin_adasyn_nosyn, Ytrain8020_bin_adasyn_nosyn = ADASYN().fit_resample(X, Ytrain8020_bin_nosyn)\n","\n","X = dvec.fit_transform(Xtrain6535_bin_nosyn.to_dict('records'))\n","Xtrain6535_bin_adasyn_nosyn, Ytrain6535_bin_adasyn_nosyn = ADASYN().fit_resample(X, Ytrain6535_bin_nosyn)\n","print(sorted(Counter(Ytrain6535_bin_adasyn_nosyn).items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFKTe7t0W3Ed"},"outputs":[],"source":["# Vectorize Test datasets as well\n","dvec = DictVectorizer(sparse=False)\n","\n","Xtest8020_bin_smote = dvec.fit_transform(Xtest8020_bin.to_dict('records'))\n","Xtest8020_bin_adasyn = dvec.fit_transform(Xtest8020_bin.to_dict('records'))\n","Xtest8020_bin_smote_nosyn = dvec.fit_transform(Xtest8020_bin_nosyn.to_dict('records'))\n","Xtest8020_bin_adasyn_nosyn = dvec.fit_transform(Xtest8020_bin_nosyn.to_dict('records'))\n","\n","Xtest6535_bin_smote = dvec.fit_transform(Xtest6535_bin.to_dict('records'))\n","Xtest6535_bin_adasyn = dvec.fit_transform(Xtest6535_bin.to_dict('records'))\n","Xtest6535_bin_smote_nosyn = dvec.fit_transform(Xtest6535_bin_nosyn.to_dict('records'))\n","Xtest6535_bin_adasyn_nosyn = dvec.fit_transform(Xtest6535_bin_nosyn.to_dict('records'))"]},{"cell_type":"markdown","metadata":{"id":"1tX3fr11SOs0"},"source":["## Import SKlearn Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9lVec5LTOQM"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import uniform, randint\n","from sklearn.pipeline import Pipeline\n","\n","from sklearn.model_selection import LeaveOneOut\n","\n","# Features preprocessing\n","from sklearn.preprocessing import MinMaxScaler # Unused...\n","from sklearn.preprocessing import StandardScaler\n","# Dummy\n","from sklearn.dummy import DummyClassifier\n","# Linear classifiers\n","from sklearn.linear_model import Perceptron\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.svm import LinearSVC, SVC\n","# Bernoulli Model (requires binary vectorizer)\n","from sklearn.naive_bayes import BernoulliNB\n","# Ensemble(+Tree) Classifiers\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Neural network classifier (will take longer time to train)\n","from sklearn.neural_network import MLPClassifier\n","\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"markdown","metadata":{"id":"oxIr7jcVXZFT"},"source":["## Machine Learning Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oT7FYVSX4uc8"},"outputs":[],"source":["scoring_metrics = ['accuracy'] #, 'roc_auc']\n","models = {}\n","datasets = {\n","    # NOTE: All datasets include both triggered and non-triggered trojans\n","    # ==========================================================================\n","    # 80%-20% Training percentage\n","    # ==========================================================================\n","    '_8020': {\n","        'Xtrain': Xtrain8020,\n","        'Ytrain': Ytrain8020,\n","        'Xtest': Xtest8020,\n","        'Ytest': Ytest8020\n","    },\n","    '_8020_nosyn': {\n","        'Xtrain': Xtrain8020_nosyn,\n","        'Ytrain': Ytrain8020_nosyn,\n","        'Xtest': Xtest8020_nosyn,\n","        'Ytest': Ytest8020_nosyn\n","    },\n","    # '_8020_trig': {\n","    #     'Xtrain': Xtrain8020_trig,\n","    #     'Ytrain': Ytrain8020_trig,\n","    #     'Xtest': Xtest8020_trig,\n","    #     'Ytest': Ytest8020_trig\n","    # },\n","    # '_8020_ntrig': {\n","    #     'Xtrain': Xtrain8020_ntrig,\n","    #     'Ytrain': Ytrain8020_ntrig,\n","    #     'Xtest': Xtest8020_ntrig,\n","    #     'Ytest': Ytest8020_ntrig\n","    # },\n","    '_8020_bin_resampled': {\n","        'Xtrain': Xtrain8020_bin_resampled,\n","        'Xtest': Xtest8020_bin,\n","        'Ytrain': Ytrain8020_bin_resampled,\n","        'Ytest': Ytest8020_bin\n","    },\n","    # '_8020_bin_smote': {\n","    #     'Xtrain': Xtrain8020_bin_smote,\n","    #     'Xtest': Xtest8020_bin_smote,\n","    #     'Ytrain': Ytrain8020_bin_smote,\n","    #     'Ytest': Ytest8020_bin\n","    # },\n","    # '_8020_bin_adasyn': {\n","    #     'Xtrain': Xtrain8020_bin_adasyn,\n","    #     'Xtest': Xtest8020_bin_adasyn,\n","    #     'Ytrain': Ytrain8020_bin_adasyn,\n","    #     'Ytest': Ytest8020_bin\n","    # },\n","    '_8020_bin_resampled_nosyn': {\n","        'Xtrain': Xtrain8020_bin_resampled_nosyn,\n","        'Xtest': Xtest8020_bin_nosyn,\n","        'Ytrain': Ytrain8020_bin_resampled_nosyn,\n","        'Ytest': Ytest8020_bin_nosyn\n","    },\n","    # '_8020_bin_smote_nosyn': {\n","    #     'Xtrain': Xtrain8020_bin_smote_nosyn,\n","    #     'Xtest': Xtest8020_bin_smote_nosyn,\n","    #     'Ytrain': Ytrain8020_bin_smote_nosyn,\n","    #     'Ytest': Ytest8020_bin_nosyn\n","    # },\n","    # '_8020_bin_adasyn_nosyn': {\n","    #     'Xtrain': Xtrain8020_bin_adasyn_nosyn,\n","    #     'Xtest': Xtest8020_bin_adasyn_nosyn,\n","    #     'Ytrain': Ytrain8020_bin_adasyn_nosyn,\n","    #     'Ytest': Ytest8020_bin_nosyn\n","    # },\n","    # ==========================================================================\n","    # 65%-35% Training percentage\n","    # ==========================================================================\n","    '_6535': {\n","        'Xtrain': Xtrain6535,\n","        'Ytrain': Ytrain6535,\n","        'Xtest': Xtest6535,\n","        'Ytest': Ytest6535\n","    },\n","    '_6535_nosyn': {\n","        'Xtrain': Xtrain6535_nosyn,\n","        'Ytrain': Ytrain6535_nosyn,\n","        'Xtest': Xtest6535_nosyn,\n","        'Ytest': Ytest6535_nosyn\n","    },\n","    # '_6535_trig': {\n","    #     'Xtrain': Xtrain6535_trig,\n","    #     'Ytrain': Ytrain6535_trig,\n","    #     'Xtest': Xtest6535_trig,\n","    #     'Ytest': Ytest6535_trig\n","    # },\n","    # '_6535_ntrig': {\n","    #     'Xtrain': Xtrain6535_ntrig,\n","    #     'Ytrain': Ytrain6535_ntrig,\n","    #     'Xtest': Xtest6535_ntrig,\n","    #     'Ytest': Ytest6535_ntrig\n","    # },\n","    '_6535_bin_resampled': {\n","        'Xtrain': Xtrain6535_bin_resampled,\n","        'Xtest': Xtest6535_bin,\n","        'Ytrain': Ytrain6535_bin_resampled,\n","        'Ytest': Ytest6535_bin\n","    },\n","    # '_6535_bin_smote': {\n","    #     'Xtrain': Xtrain6535_bin_smote,\n","    #     'Xtest': Xtest6535_bin_smote,\n","    #     'Ytrain': Ytrain6535_bin_smote,\n","    #     'Ytest': Ytest6535_bin\n","    # },\n","    # '_6535_bin_adasyn': {\n","    #     'Xtrain': Xtrain6535_bin_adasyn,\n","    #     'Xtest': Xtest6535_bin_adasyn,\n","    #     'Ytrain': Ytrain6535_bin_adasyn,\n","    #     'Ytest': Ytest6535_bin\n","    # },\n","    '_6535_bin_resampled_nosyn': {\n","        'Xtrain': Xtrain6535_bin_resampled_nosyn,\n","        'Xtest': Xtest6535_bin_nosyn,\n","        'Ytrain': Ytrain6535_bin_resampled_nosyn,\n","        'Ytest': Ytest6535_bin_nosyn\n","    },\n","    # '_6535_bin_smote_nosyn': {\n","    #     'Xtrain': Xtrain6535_bin_smote_nosyn,\n","    #     'Xtest': Xtest6535_bin_smote_nosyn,\n","    #     'Ytrain': Ytrain6535_bin_smote_nosyn,\n","    #     'Ytest': Ytest6535_bin_nosyn\n","    # },\n","    # '_6535_bin_adasyn_nosyn': {\n","    #     'Xtrain': Xtrain6535_bin_adasyn_nosyn,\n","    #     'Xtest': Xtest6535_bin_adasyn_nosyn,\n","    #     'Ytrain': Ytrain6535_bin_adasyn_nosyn,\n","    #     'Ytest': Ytest6535_bin_nosyn\n","    # },\n","}"]},{"cell_type":"markdown","metadata":{"id":"dKXkpUjTXDfE"},"source":["### Dummy Classifier (Baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L63az8CDXGjI"},"outputs":[],"source":["def make_model():\n","    # Setup the pipeline with the vectorizer and the classifier\n","    pipeline = Pipeline([\n","        ('vect', DictVectorizer(sparse=False)),\n","        ('clf', DummyClassifier()),\n","    ])\n","    # Add the classifier configurations to the grid of parameters\n","    parameters = {\n","        'clf__strategy': ('most_frequent', 'prior'),\n","    }\n","    return GridSearchCV(pipeline, parameters,\n","                        scoring=scoring_metrics,\n","                        refit='accuracy',\n","                        cv=5,\n","                        error_score='raise',\n","                        n_jobs=-1,\n","                        verbose=3)\n","\n","for dataset_type in datasets.keys():\n","    # Add the grid-search to the list of models\n","    model_name = 'Dummy'\n","    models[f'{model_name}{dataset_type}'] = {}\n","    # NOTE: For the resampled datasets, the train data are already vectorized \n","    if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","        models[f'{model_name}{dataset_type}']['model'] = make_model()\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","        models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest']\n","    else:\n","        models[f'{model_name}{dataset_type}']['model'] = make_model()\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","        models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","    models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"zDM-iUxdXiRx"},"source":["### Decision Tree Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4l60GalXXhAq"},"outputs":[],"source":["def make_model():\n","    # Setup the pipeline with the vectorizer and the classifier\n","    pipeline = Pipeline([\n","        ('vect', DictVectorizer(sparse=False)),\n","        ('clf', DecisionTreeClassifier()),\n","    ])\n","    # Add the classifier configurations to the grid of parameters\n","    parameters = {\n","        'clf__max_depth': randint(32, 256),\n","    }\n","    return  RandomizedSearchCV(pipeline,\n","                               parameters,\n","                               scoring=scoring_metrics,\n","                               refit='accuracy',\n","                               cv=5,\n","                               error_score='raise',\n","                               n_jobs=-1,\n","                               random_state=0,\n","                               verbose=3)\n","\n","for dataset_type in datasets.keys():\n","    # Add the grid-search to the list of models\n","    model_name = 'DecisionTree'\n","    models[f'{model_name}{dataset_type}'] = {}\n","    models[f'{model_name}{dataset_type}']['model'] = make_model()\n","    if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","    else:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","    models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"O0QEx505_PPG"},"source":["### Random Forest Classifier\n","\n","For some unkown reason, the cross-validation of random forests with `RandomizedSearchCV` takes a long time to finish and returns zero scores. Because of that, I'm wrapping a _fixed_ pipeline with a class handling the required methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Db91Xlqx_SeZ"},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","import numpy as np\n","\n","class RFClassifier(object):\n","    def __init__(self):\n","        parameters = {\n","            'clf__n_estimators': randint(100, 128),\n","            'clf__max_depth': randint(64, 256),\n","        }\n","        self.best_estimator_ = Pipeline([\n","                                        ('vect', DictVectorizer(sparse=False)),\n","                                        ('clf', RandomForestClassifier(n_estimators=256, max_depth=128)),\n","                                    ])\n","        # self.best_estimator_ = RandomizedSearchCV(pipeline,\n","        #                                           parameters,\n","        #                                           scoring=scoring_metrics,\n","        #                                           refit='accuracy',\n","        #                                           cv=5,\n","        #                                           n_iter=10,\n","        #                                           error_score='raise',\n","        #                                           n_jobs=-1,\n","        #                                           random_state=0,\n","        #                                           verbose=3)\n","        self.best_score_ = None\n","        self.classes_ = hth_classes\n","\n","    def fit(self, X, Y):\n","        self.cv_results_ = cross_validate(self.best_estimator_, X, y=Y,\n","                                              scoring='accuracy',\n","                                              cv=5,\n","                                              n_jobs=-1)\n","        self.best_score_ = np.mean(self.cv_results_['test_score'])\n","        self.best_estimator_.fit(X, Y)\n","\n","    def score(self, X, Y):\n","        return self.best_estimator_.score(X, Y)\n","    \n","    def predict(self, X):\n","        return self.best_estimator_.predict(X)\n","\n","for dataset_type in datasets.keys():\n","    # Add the grid-search to the list of models\n","    model_name = 'RandomForest'\n","    models[f'{model_name}{dataset_type}'] = {}\n","    models[f'{model_name}{dataset_type}']['model'] = make_model()\n","    # NOTE: For the resampled datasets, the train data are already vectorized \n","    if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","    else:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","    models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"zF-PgESLfMr3"},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOqeXDibfPTP"},"outputs":[],"source":["def make_model():\n","    # Setup the pipeline with the vectorizer and the classifier\n","    pipeline = Pipeline([\n","        ('vect', DictVectorizer(sparse=False)),\n","        ('clf', LogisticRegression(max_iter=500)),\n","    ])\n","    # Add the classifier configurations to the grid of parameters\n","    parameters = {\n","        'clf__C': uniform(0.6, 0.95),\n","        'clf__solver': ('newton-cg', 'lbfgs', 'liblinear'),\n","    }\n","    # Add the grid-search to the list of models\n","    return RandomizedSearchCV(pipeline,\n","                              parameters,\n","                              scoring=scoring_metrics,\n","                              refit='accuracy',\n","                              cv=5,\n","                              error_score='raise',\n","                              n_jobs=-1,\n","                              random_state=0,\n","                              verbose=3)\n","\n","for dataset_type in datasets.keys():\n","    # Add the grid-search to the list of models\n","    model_name = 'LogisticRegression'\n","    models[f'{model_name}{dataset_type}'] = {}\n","    models[f'{model_name}{dataset_type}']['model'] = make_model()\n","    # NOTE: For the resampled datasets, the train data are already vectorized \n","    if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","    else:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","    models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"_q9P0LVWJMNA"},"source":["### Linear SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTXzKYTnJKCG"},"outputs":[],"source":["def make_model():\n","    # Setup the pipeline with the vectorizer and the classifier\n","    pipeline = Pipeline([\n","        ('vect', DictVectorizer(sparse=False)),\n","        ('clf', LinearSVC(dual=True, max_iter=2000)),\n","    ])\n","    # Add the classifier configurations to the grid of parameters\n","    parameters = {\n","        'clf__loss': ('hinge', 'squared_hinge'),\n","        'clf__C': uniform(0.4, 0.95),\n","    }\n","    # Add the grid-search to the list of models\n","    return RandomizedSearchCV(pipeline,\n","                              parameters,\n","                              scoring=scoring_metrics,\n","                              refit='accuracy',\n","                              cv=5,\n","                              error_score='raise',\n","                              n_jobs=-1,\n","                              random_state=0,\n","                              verbose=3)\n","\n","for dataset_type in datasets.keys():\n","    # Add the grid-search to the list of models\n","    model_name = 'LinearSVC'\n","    models[f'{model_name}{dataset_type}'] = {}\n","    models[f'{model_name}{dataset_type}']['model'] = make_model()\n","    # NOTE: For the resampled datasets, the train data are already vectorized \n","    if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","    else:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","    models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"OHKcmmO2YDlN"},"source":["###  Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2NHHQi8YHX6"},"outputs":[],"source":["def make_model():\n","    # Setup the pipeline with the vectorizer and the classifier\n","    pipeline = Pipeline([\n","        ('vect', DictVectorizer(sparse=False)),\n","        ('clf', GradientBoostingClassifier(max_depth=8)),\n","    ])\n","    # Add the classifier configurations to the grid of parameters\n","    parameters = {\n","        'clf__n_estimators': randint(80, 120),\n","        'clf__learning_rate': uniform(0.01, 0.1),\n","    }\n","    # Add the grid-search to the list of models\n","    return RandomizedSearchCV(pipeline,\n","                              parameters,\n","                              scoring=scoring_metrics,\n","                              refit='accuracy',\n","                              cv=5,\n","                              error_score='raise',\n","                              n_jobs=-1,\n","                              random_state=0,\n","                              verbose=3)\n","\n","for dataset_type in datasets.keys():\n","    # Add the grid-search to the list of models\n","    model_name = 'GradientBoosting'\n","    models[f'{model_name}{dataset_type}'] = {}\n","    models[f'{model_name}{dataset_type}']['model'] = make_model()\n","    # NOTE: For the resampled datasets, the train data are already vectorized \n","    if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","    else:\n","        models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","    models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","    models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"zR_eygyY-3nz"},"source":["## Deep Learning Models\n"]},{"cell_type":"markdown","metadata":{"id":"Wg8jP264KPUJ"},"source":["### MLP Classifier with SKlearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDM8bmPZKR1y"},"outputs":[],"source":["# from scipy.stats import randint as sp_randint\n","\n","# def make_model():\n","#     # Setup the pipeline with the vectorizer and the classifier\n","#     pipeline = Pipeline([\n","#         ('vect', DictVectorizer(sparse=False)),\n","#         ('norm', StandardScaler()),\n","#         ('clf', MLPClassifier(learning_rate_init=0.01, max_iter=500)),\n","#     ])\n","#     # Add the classifier configurations to the grid of parameters\n","\n","#     # TODO: How do I randomize the layer number search???\n","#     parameters = {\n","#         'clf__hidden_layer_sizes': [(128, 128),(128, 128, 128),],\n","#     }\n","#     # Add the grid-search to the list of models\n","#     return GridSearchCV(pipeline,\n","#                         parameters,\n","#                         scoring=scoring_metrics,\n","#                         refit='accuracy',\n","#                         cv=5,\n","#                         error_score='raise',\n","#                         n_jobs=-1,\n","#                         verbose=3)\n","\n","# for dataset_type in datasets.keys():\n","#     # Add the grid-search to the list of models\n","#     model_name = 'MLP'\n","#     models[f'{model_name}{dataset_type}'] = {}\n","#     models[f'{model_name}{dataset_type}']['model'] = make_model()\n","#     # NOTE: For the resampled datasets, the train data are already vectorized \n","#     if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","#         models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","#     else:\n","#         models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","#     models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","#     models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","#     models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"markdown","metadata":{"id":"m20gdybR1lwv"},"source":["### MLP Classifier with Keras\n","\n","In order to fully exploit Sklearn developing flow, we utilize Keras model wrapper provided by `scikeras`.\n","\n","The idea is to define a custom Keras model and integrate it into a Sklearn pipeline. In this way, we don't need to worry about fine-grain details like input/output shape matching and we can exploit the CV search.\n","\n","More information and tutorials can be found [here](https://www.adriangb.com/scikeras/stable/notebooks/MLPClassifier_MLPRegressor.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zhyeugZ1jgQ"},"outputs":[],"source":["# from typing import Dict, Iterable, Any\n","\n","# def get_clf_model(num_hidden_layers: int,\n","#                   hidden_layer_sizes: int,\n","#                   meta: Dict[str, Any],\n","#                   compile_kwargs: Dict[str, Any]):\n","#     # Generate a model and populate it according to the passed arguments\n","#     model = keras.Sequential()\n","#     inp = keras.layers.Input(shape=(meta['n_features_in_']))\n","#     model.add(inp)\n","#     for _ in range(num_hidden_layers):\n","#         l2 = tf.keras.regularizers.l2(0.001)\n","#         layer = Dense(hidden_layer_sizes, activation='relu', kernel_regularizer=l2)\n","#         model.add(layer)\n","#     # The final layer and the loss function will depend on the kind of task we\n","#     # are aiming for (Sklearn will automatically determine it and pass the\n","#     # information via the `meta` dictionary).\n","#     if meta['target_type_'] == 'binary':\n","#         n_output_units = 1\n","#         output_activation = 'sigmoid'\n","#         loss = 'binary_crossentropy'\n","#     elif meta['target_type_'] == 'multiclass':\n","#         n_output_units = meta['n_classes_']\n","#         output_activation = 'softmax'\n","#         loss = 'sparse_categorical_crossentropy'\n","#     else:\n","#         raise NotImplementedError(f'Unsupported task type: {meta[\"target_type_\"]}')\n","#     # Add the final layer and compile the model\n","#     out = Dense(n_output_units, activation=output_activation)\n","#     model.add(out)\n","#     model.compile(loss=loss, optimizer=compile_kwargs['optimizer'])\n","#     return model\n","\n","# def make_model():\n","#     # Setup the pipeline with the vectorizer and the classifier\n","#     # NOTE: We are using a Sklearn wrapper for the Keras model\n","#     pipeline = Pipeline([\n","#         ('vect', DictVectorizer(sparse=False)),\n","#         ('norm', StandardScaler()),\n","#         ('clf', KerasClassifier(model=get_clf_model,\n","#                                 optimizer='adam',\n","#                                 optimizer__learning_rate=0.1,\n","#                                 epochs=50,\n","#                                 batch_size=32)),\n","#     ])\n","#     # NOTE: Passing specific parameters to the model generation function should\n","#     # be done by specifying the \"model__\" keyword.\n","#     parameters = {\n","#         'clf__model__hidden_layer_sizes': randint(128, 256),\n","#         'clf__model__num_hidden_layers': randint(2, 3),\n","#     }\n","#     return RandomizedSearchCV(pipeline,\n","#                               parameters,\n","#                               scoring=scoring_metrics,\n","#                               refit='accuracy',\n","#                               cv=5,\n","#                               error_score='raise',\n","#                               n_jobs=-1,\n","#                               random_state=0,\n","#                               verbose=3)\n","\n","# for dataset_type in datasets.keys():\n","#     # Add the grid-search to the list of models\n","#     model_name = 'MLPKeras'\n","#     models[f'{model_name}{dataset_type}'] = {}\n","#     models[f'{model_name}{dataset_type}']['model'] = make_model()\n","#     # NOTE: For the resampled datasets, the train data are already vectorized \n","#     if '_smote' in dataset_type or '_adasyn' in dataset_type:\n","#         models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain']\n","#     else:\n","#         models[f'{model_name}{dataset_type}']['Xtrain'] = datasets[dataset_type]['Xtrain'].to_dict('records')\n","#     models[f'{model_name}{dataset_type}']['Ytrain'] = datasets[dataset_type]['Ytrain']\n","#     models[f'{model_name}{dataset_type}']['Xtest'] = datasets[dataset_type]['Xtest'].to_dict('records')\n","#     models[f'{model_name}{dataset_type}']['Ytest'] = datasets[dataset_type]['Ytest']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgBt9Y-_1hjA"},"outputs":[],"source":["# from __future__ import print_function\n","# import tensorflow as tf\n","# import keras\n","# from keras.models import Sequential\n","# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","# from keras import Input\n","\n","# from sklearn.model_selection import RepeatedKFold, cross_val_score\n","# from scikeras.wrappers import KerasClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ypzgr3mVCLOj"},"outputs":[],"source":["# %%capture\n","# try:\n","#     import scikeras\n","# except ImportError:\n","#     !python -m pip install scikeras"]},{"cell_type":"markdown","metadata":{"id":"5HoKqAfSYXM9"},"source":["## Cross-Validation and Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":659,"status":"ok","timestamp":1664739322464,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"ykoPF6naYZ_R","outputId":"d498f73f-ab7f-4454-a4ab-2366cce71c7b"},"outputs":[],"source":["from sklearn.utils.validation import check_is_fitted\n","from joblib import dump, load\n","\n","SCORE_THRESHOLD = 0.0\n","LOAD_FROM_DISK = True\n","\n","scores = {}\n","for model_type in models.keys():\n","    modelfile = os.path.join(models_dir, f'{model_type}.joblib')\n","    if LOAD_FROM_DISK:\n","        # Check if model exists on disk and eventually load it.\n","        if os.path.exists(modelfile):\n","            models[model_type]['model'] = load(modelfile)\n","            print(f'Successfully loaded model: {model_type}')\n","            model_saved = True\n","        else:\n","            model_saved = False\n","    else:\n","        model_saved = False\n","    # Check if model is fitted and if so, that its score is above a threshold.\n","    try:\n","        model = models[model_type]['model']\n","        x_test = models[model_type]['Xtest']\n","        y_test = models[model_type]['Ytest']\n","        check_is_fitted(model)\n","        model_trained = model.score(x_test, y_test) > SCORE_THRESHOLD\n","        print(f'Model {model_type} not achieving high score, attempting retraining.')\n","    except:\n","        print(f'Model {model_type} not fitted.')\n","        model_trained = False\n","    # Fit if the performance of the model are low or if it has not been loaded\n","    # from disk.\n","    if not model_trained or not model_saved:\n","        print(f'INFO. Fitting model: {model_type}')\n","        model = models[model_type]['model']\n","        x_train = models[model_type]['Xtrain']\n","        y_train = models[model_type]['Ytrain']\n","        model.fit(x_train, y_train)\n","        dump(model, modelfile)\n","    scores[model_type] = score = models[model_type]['model'].best_score_\n","    print(f'INFO. {model_type} model reached CV score of: {score:.3f}')\n","    print('-' * 80)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1664737080804,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"XMMIM5MNYg3p","outputId":"f5503a2b-4edf-4f55-d704-58204975db25"},"outputs":[],"source":["table_data = []\n","for model_type in models.keys():\n","    table_data.append((model_type, models[model_type]['model'].best_score_))\n","pd.DataFrame(table_data, columns=['Model', 'CV Score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"executionInfo":{"elapsed":830,"status":"ok","timestamp":1664737081621,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"8sPoWAlIaOJH","outputId":"d1186840-80e7-489f-fb5d-f7fba0e68fa7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","def clean_name(s):\n","    return s.replace('_bin_resampled', '') \\\n","            .replace('_8020', ' 80%-20%') \\\n","            .replace('_6535', ' 65%-35%') \\\n","            .replace('_nosyn', '')\n","\n","full_scores = [scores[k] for k in models.keys() if '_nosyn' not in k and '_bin' not in k]\n","nosyn_scores = [scores[k] for k in models.keys() if '_nosyn' in k and '_bin' not in k]\n","keys = [clean_name(k) for k in models.keys() if '_nosyn' not in k and '_bin' not in k]\n","\n","x = np.array([x for x in range(len(keys))])\n","plt.bar(x, full_scores, color=['C1' if s > 0.8 else 'C0' for s in full_scores])\n","# plt.bar(-x, full_scores, color='C0', label='PF + SF')\n","# plt.bar(x, nosyn_scores, color='C1', label='SF')\n","\n","plt.xticks(x, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","\n","plt.ylabel('Accuracy')\n","# plt.title('CV Accuracy score of the selected classifiers')\n","plt.tight_layout()\n","# plt.legend()\n","plt.savefig(os.path.join(fig_dir, f'cv_accuracy.pdf'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1413,"status":"ok","timestamp":1664737083024,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"YNBTaJAllo2I","outputId":"2d463879-0784-4bb8-83c6-c3ac91f10141"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","# score_val = scores.values()\n","# keys = models.keys()\n","score_val = [scores[k] for k in models.keys() if '_nosyn' not in k and '_bin' in k]\n","keys = [clean_name(k) for k in models.keys() if '_nosyn' not in k and '_bin' not in k]\n","\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.8 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","\n","plt.ylabel('Accuracy')\n","plt.title('CV Accuracy score of the selected binary classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'cv_accuracy_bin.pdf'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":594,"status":"ok","timestamp":1664737083616,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"Nn-Svh9d1WtS","outputId":"22098032-e8dc-4166-fa5e-661c6972610e"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","# score_val = scores.values()\n","# keys = models.keys()\n","score_val = [scores[k] for k in models.keys() if '_nosyn' in k and '_bin' not in k]\n","keys = [clean_name(k) for k in models.keys() if '_nosyn' in k and '_bin' not in k]\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.8 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylim([0, 1])\n","\n","plt.ylabel('Accuracy')\n","plt.title('CV Accuracy score of the selected classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'cv_accuracy_nosyn.pdf'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":603,"status":"ok","timestamp":1664737084211,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"omSoTDyOl4tf","outputId":"16ea6304-7076-467b-aefd-99262f57f818"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","score_val = [scores[k] for k in models.keys() if '_nosyn' in k and '_bin' in k]\n","keys = [clean_name(k) for k in models.keys() if '_nosyn' in k and '_bin' in k]\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.8 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylim([0, 1])\n","\n","plt.ylabel('Accuracy')\n","plt.title('CV Accuracy score of the selected binary classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'cv_accuracy_nosyn_bin.pdf'))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cgBWCuEqLnjl"},"source":["### Inspect Best Models from Random Grid Search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIdyjXjWIcwq"},"outputs":[],"source":["# if 'MLPKeras_base' in models:\n","#     models['MLPKeras_base']['model'].best_estimator_['clf'].model_.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hg4MMsupwXOs"},"outputs":[],"source":["# if 'RandomForest_base' in models:\n","#     models['RandomForest_base']['model'].best_estimator_['clf'].classes_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlB6PdoSPGoX"},"outputs":[],"source":["# if 'RandomForest_base' in models:\n","#     models['RandomForest_base']['model'].cv_results_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYbV6uNn-LcL"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.tree import plot_tree\n","\n","if 'RandomForest_base' in models:\n","    tree = models['RandomForest_base']['model'].best_estimator_['clf'].estimators_[0]\n","    plot_tree(tree, \n","            #   feature_names=dv.feature_names_,\n","            class_names=hth_classes,\n","            filled=True, rounded=True)\n","    plt.savefig(os.path.join(fig_dir, f'tree.pdf'))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1tJWsDvYjqG"},"outputs":[],"source":["# import numpy as np\n","\n","# def get_roc_auc(model_type):\n","#     i = np.argmin(models[model_type].cv_results_['rank_test_roc_auc'])\n","#     return models[model_type].cv_results_['mean_test_roc_auc'][i]\n","\n","# roc_auc_scores = {}\n","# for model_type in models.keys():\n","#     roc_auc_scores[model_type] = get_roc_auc(model_type)\n","#     print(f'{model_type} ROC-AUC CV score: {get_roc_auc(model_type):.3f}')\n","\n","# linspace = [x for x in range(len(models.keys()))]\n","# plt.bar(linspace, roc_auc_scores.values())\n","# plt.xticks(linspace, [f'{m}' for m in models.keys()], rotation=90)\n","# plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","# plt.ylabel('Accuracy')\n","# plt.title('ROC-AUC CV Accuracy score of different classifiers')\n","# plt.savefig(os.path.join(data_dir, f'cv_roc_auc.pdf'))\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZcwpRBbRZAKR"},"source":["## Evaluation\n","\n","First collect all the scores, then plot and display them accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9BeUFvPZEql"},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support\n","\n","ROUNDING = 2\n","train_test_split = {\n","    '8020': '80%-20%',\n","    '6535': '65%-35%',\n","}\n","table_data = []\n","accuracy_scores = {}\n","for model_type in models.keys():\n","    model = models[model_type]['model']\n","    x_test = models[model_type]['Xtest']\n","    y_test = models[model_type]['Ytest']\n","    cv_accuracy = round(model.best_score_, ROUNDING)\n","    y_pred = model.predict(x_test)\n","    accuracy = round(model.score(x_test, y_test), ROUNDING)\n","    accuracy_str = f'{accuracy:.4f}'\n","    m_str = model_type\n","    if '_bin' in model_type:\n","        binary_classification = True\n","    else:\n","        binary_classification = False\n","    if '_nosyn' in model_type:\n","        # continue\n","        included_featues = 'IF'\n","        m_str = m_str.replace('_nosyn', '')\n","    else:\n","        included_featues = 'PF + IF'\n","    # Get information about the model type and extract reporting model ID label\n","    if '_ntrig' in model_type:\n","        triggered_dataset = 'False'\n","        m_str = m_str.replace('_ntrig', '')\n","        train_perc = m_str.split('_')[1]\n","        model_str = m_str.split('_')[0]\n","        dummy_type = '_' + train_perc if triggered_dataset != 'Both' else ''\n","        dummy_type += '_ntrig'\n","    elif '_trig' in model_type:\n","        triggered_dataset = 'True'\n","        m_str = m_str.replace('_trig', '')\n","        train_perc = m_str.split('_')[1]\n","        model_str = m_str.split('_')[0]\n","        dummy_type = '_' + train_perc if triggered_dataset != 'Both' else ''\n","        dummy_type += '_trig'\n","    else:\n","        triggered_dataset = 'Both'\n","        train_perc = m_str.split('_')[1]\n","        model_str = m_str.split('_')[0]\n","        dummy_type = '_' + train_perc\n","    dummy_type += '_nosyn' if '_nosyn' in model_type else ''\n","    # Compute additional scores\n","    other_scores = precision_recall_fscore_support(y_test, y_pred,\n","                                                    average='micro',\n","                                                    # labels=hth_classes,\n","                                                    zero_division=0)\n","    # Do not consider the support, i.e. the last score\n","    other_scores = [round(s, ROUNDING) if s is not None else 0 for s in other_scores]\n","    # Calculate Cohen's Kappa\n","    if 'Dummy' not in model_type:\n","        dummy_score = models[f'Dummy{dummy_type}']['model'].score(x_test, y_test)\n","        cohens_kappa = (accuracy - dummy_score) / (1.0 - dummy_score)\n","        cohens_kappa = round(cohens_kappa, ROUNDING)\n","    else:\n","        cohens_kappa = 0\n","    model_str_split = f'{model_str} ({train_test_split[train_perc]})'\n","    table_entry = (model_str, model_str_split, train_test_split[train_perc],\n","                   included_featues, triggered_dataset, binary_classification,\n","                   cv_accuracy, accuracy, *other_scores, cohens_kappa)\n","    table_data.append(table_entry)\n","    accuracy_scores[model_type] = accuracy\n","\n","cols = ['Model', 'Model and Split', 'Train/Test Split', 'Included Features',\n","        'Include Triggered', 'Binary C.', 'CV Accuracy', 'Accuracy',\n","        'Precision', 'Recall', 'Fbeta', 'Support', 'k']\n","table = pd.DataFrame(table_data, columns=cols).drop(['Support'], axis=1)\n","table.to_csv(os.path.join(data_dir, f'results.csv'), encoding='utf-8', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1664737085616,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"_wBQXKN4obEB","outputId":"c1a350f9-95cf-4719-999b-e0dbd8978ad5"},"outputs":[],"source":["table"]},{"cell_type":"markdown","metadata":{"id":"vLmdA6f10h6c"},"source":["### Score Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gvBtSsWPjX6"},"outputs":[],"source":["import matplotlib\n","\n","# data = table[table['Binary C.'] == False]\n","\n","# fig, ax =plt.subplots(1, 2)\n","# sns.barplot(data=data, y='Model', x='Accuracy', hue='Included Features', orient='h', ax=ax[0])\n","# sns.barplot(data=data, y='Model', x='k', hue='Included Features', orient='h', ax=ax[1])\n","# sns.move_legend(ax[1], 'upper left', bbox_to_anchor=(1, 1))\n","\n","# for container in ax[0].containers:\n","#     ax[0].bar_label(container, padding=10)\n","\n","# for container in ax[1].containers:\n","#     ax[1].bar_label(container, padding=10)\n","\n","# plt.grid(axis='x')\n","# plt.xlim([0, 1.1])\n","# fig.savefig(os.path.join(fig_dir, f'eval_scores.pdf'), bbox_inches='tight')\n","# fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1664737086095,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"5mO8V7g-E7d4","outputId":"d92c06d4-e678-42d6-97bb-b0041cf50c0f"},"outputs":[],"source":["data = table[table['Binary C.'] == False]\n","ax = sns.barplot(data=data, y='Model', x='CV Accuracy', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","plt.grid(axis='x')\n","plt.xlim([0, 1.1])\n","plt.savefig(os.path.join(fig_dir, f'cv_accuracy.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":1774,"status":"ok","timestamp":1664737087861,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"sWP22YfSC6D7","outputId":"953462d4-dba3-49b5-8f25-e87880838e6d"},"outputs":[],"source":["data = table[table['Binary C.'] == False]\n","ax = sns.barplot(data=data, y='Model and Split', x='Accuracy', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","plt.grid(axis='x')\n","plt.xlim([0, 1.1])\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_split.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1664737088913,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"6X-o6HgrUr_6","outputId":"e1aeb981-be79-4574-d5ea-fd33df2f2b67"},"outputs":[],"source":["data = table[table['Binary C.'] == False]\n","ax = sns.barplot(data=data, y='Model', x='Accuracy', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","plt.grid(axis='x')\n","plt.xlim([0, 1.15])\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":1436,"status":"ok","timestamp":1664737090347,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"p_hwGEA1Qaia","outputId":"46a16209-54f9-418e-82b6-055985a11bc0"},"outputs":[],"source":["data = table[table['Binary C.'] == False]\n","ax = sns.barplot(data=data, y='Model', x='k', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","plt.grid(axis='x')\n","plt.xlim([0, 1.1])\n","plt.savefig(os.path.join(fig_dir, f'eval_k.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":1400,"status":"ok","timestamp":1664737091744,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"zKNWAnFqfmWm","outputId":"bee11519-3616-426d-e116-f04b2f65189b"},"outputs":[],"source":["data = table[table['Binary C.'] == True]\n","ax = sns.barplot(data=data, y='Model', x='Fbeta', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","ax.grid(axis='x')\n","plt.xlim([0, 1.1])\n","\n","fig = ax.get_figure()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_bin_Fbeta.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":1081,"status":"ok","timestamp":1664737092819,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"8EEb_YacTEds","outputId":"26a1cfd1-d981-4eed-bec8-abe9e0c47c3e"},"outputs":[],"source":["data = table[table['Binary C.'] == True]\n","ax = sns.barplot(data=data, y='Model', x='Accuracy', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","ax.grid(axis='x')\n","plt.xlim([0, 1.1])\n","\n","fig = ax.get_figure()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_bin.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":1591,"status":"ok","timestamp":1664737094407,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"Yr44kJkZVAe3","outputId":"eb518999-b077-46db-a880-b8d7aae5b51b"},"outputs":[],"source":["data = table[table['Binary C.'] == True]\n","ax = sns.barplot(data=data, y='Model and Split', x='Accuracy', hue='Included Features', orient='h')\n","# sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n","sns.move_legend(ax, 'lower left', bbox_to_anchor=(0, 1, 1, 0))\n","\n","for container in ax.containers:\n","    ax.bar_label(container, padding=10)\n","\n","ax.grid(axis='x')\n","plt.xlim([0, 1.1])\n","\n","fig = ax.get_figure()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_bin_split.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WLOUgTO40kuA"},"source":["### ROC-AUC Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":32535,"status":"ok","timestamp":1664739086985,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"P7FDeTBOOKsT","outputId":"e3dc2493-c669-44aa-e977-3a60b1c31edb"},"outputs":[],"source":["import sklearn.metrics as metrics\n","from sklearn.calibration import CalibratedClassifierCV\n","\n","roc_curves = []\n","# Calculate the FPR and TPR for all thresholds of the classification\n","for model_type in models.keys():\n","    if '_bin' not in model_type:\n","        continue\n","    model = models[model_type]['model']\n","    x_test = models[model_type]['Xtest']\n","    y_test = models[model_type]['Ytest']\n","\n","    try:\n","        probs = model.predict_proba(x_test)\n","        preds = probs[:, 1]\n","        fpr, tpr, threshold = metrics.roc_curve(y_test, preds, pos_label='with_hth')\n","        roc_auc = metrics.auc(fpr, tpr)\n","        roc_curves.append((model_type, fpr, tpr, roc_auc))\n","    except Exception as e:\n","        print(f'* Model {model_type} raised: {e}')\n","\n","        if 'LinearSVC' in model_type:\n","            clf = CalibratedClassifierCV(model)\n","            clf.fit(models[model_type]['Xtrain'], models[model_type]['Ytrain'])\n","            y_proba = clf.predict_proba(x_test)\n","            probs = clf.predict_proba(x_test)\n","            preds = probs[:, 1]\n","            fpr, tpr, threshold = metrics.roc_curve(y_test, preds, pos_label='with_hth')\n","            roc_auc = metrics.auc(fpr, tpr)\n","            roc_curves.append((model_type, fpr, tpr, roc_auc))\n","        continue\n","\n","for model_type, fpr, tpr, roc_auc in roc_curves:\n","    if '_nosyn' in model_type and '_8020' in model_type:\n","        model_type = model_type.replace('_8020', '')\n","        plt.plot(fpr, tpr, label=f'{clean_name(model_type)} - AUC={roc_auc:0.2f}')\n","# plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","# plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([-0.04, 1.04])\n","plt.ylim([-0.04, 1.04])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.grid('both')\n","plt.savefig(os.path.join(fig_dir, f'roc-auc_nosyn_8020.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":1100,"status":"ok","timestamp":1664737096484,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"n57C2YQtSkgI","outputId":"b6fda11b-4656-407e-e7d7-70c9ce1ed1e6"},"outputs":[],"source":["for model_type, fpr, tpr, roc_auc in roc_curves:\n","    if '_nosyn' in model_type and '_6535' in model_type:\n","        model_type = model_type.replace('_6535', '')\n","        plt.plot(fpr, tpr, label=f'{clean_name(model_type)} - AUC={roc_auc:0.2f}')\n","# plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","# plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([-0.04, 1.04])\n","plt.ylim([-0.04, 1.04])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.grid('both')\n","plt.savefig(os.path.join(fig_dir, f'roc-auc_nosyn_6535.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":513,"status":"ok","timestamp":1664737096987,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"g_5FLbXfS16m","outputId":"297f0dec-51b5-425a-8d87-a8e1e9522cb1"},"outputs":[],"source":["for model_type, fpr, tpr, roc_auc in roc_curves:\n","    if '_nosyn' not in model_type and '_8020' in model_type:\n","        model_type = model_type.replace('_8020', '')\n","        plt.plot(fpr, tpr, label=f'{clean_name(model_type)} - AUC={roc_auc:0.2f}')\n","# plt.title('Receiver Operating Characteristic')\n","plt.legend(loc='lower right')\n","# plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([-0.04, 1.04])\n","plt.ylim([-0.04, 1.04])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.grid('both')\n","plt.savefig(os.path.join(fig_dir, f'roc-auc_8020.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"elapsed":1114,"status":"ok","timestamp":1664737098094,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"r8WTuCwhS8_F","outputId":"740bf892-ccb8-47f3-9fd8-552066a957b6"},"outputs":[],"source":["for model_type, fpr, tpr, roc_auc in roc_curves:\n","    if '_nosyn' not in model_type and '_6535' in model_type:\n","        model_type = model_type.replace('_6535', '')\n","        plt.plot(fpr, tpr, label=f'{clean_name(model_type)} - AUC={roc_auc:0.2f}')\n","# plt.title('Receiver Operating Characteristic')\n","plt.legend(loc = 'lower right')\n","# plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([-0.04, 1.04])\n","plt.ylim([-0.04, 1.04])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.grid('both')\n","plt.savefig(os.path.join(fig_dir, f'roc-auc_6535.pdf'), bbox_inches='tight')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lhNt4CVfZB8H"},"source":["### Accuracy Plots (Deprecated)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":940,"status":"ok","timestamp":1664737099031,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"3qvcpzXfH_mi","outputId":"401bd28e-9f42-4fbf-b198-3c0cbc0682f2"},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","# score_val = accuracy_scores.values()\n","# keys = models.keys()\n","score_val = [accuracy_scores[k] for k in models.keys() if '_nosyn' not in k and '_bin' not in k]\n","keys = [k for k in models.keys() if '_nosyn' not in k and '_bin' not in k]\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.75 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy score of the selected classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy.pdf'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1400,"status":"ok","timestamp":1664737100428,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"R15CQmaHh9Pp","outputId":"25a2ba4e-97aa-4742-e76a-4457cfda4cb2"},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","# score_val = accuracy_scores.values()\n","# keys = models.keys()\n","score_val = [accuracy_scores[k] for k in models.keys() if '_nosyn' not in k and '_bin' in k]\n","keys = [k.strip('_bin_resampled') for k in models.keys() if '_nosyn' not in k and '_bin' in k]\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.75 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy score of the selected binary classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_bin.pdf'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1386,"status":"ok","timestamp":1664737101812,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"G2UGL4Xk1DLS","outputId":"52acf0a8-3eed-451b-fb95-59614694e377"},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","# score_val = accuracy_scores.values()\n","# keys = models.keys()\n","score_val = [accuracy_scores[k] for k in models.keys() if '_nosyn' in k and '_bin' not in k]\n","keys = [k.replace('_bin_resampled', '') for k in models.keys() if '_nosyn' in k and '_bin' not in k]\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.75 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.ylim([0, 1])\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy score of the selected classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_nosyn.pdf'))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1739,"status":"ok","timestamp":1664737103541,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"CsNTpJCkiyt9","outputId":"de6ccd05-3416-49ac-f374-ec7cf3114de6"},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","# score_val = accuracy_scores.values()\n","# keys = models.keys()\n","score_val = [accuracy_scores[k] for k in models.keys() if '_nosyn' in k and '_bin' in k]\n","keys = [k.replace('_bin_resampled', '') for k in models.keys() if '_nosyn' in k and '_bin' in k]\n","\n","linspace = [x for x in range(len(keys))]\n","plt.bar(linspace, score_val, color=['C1' if s > 0.75 else 'C0' for s in score_val])\n","plt.xticks(linspace, [f'{m}' for m in keys], rotation=90)\n","\n","# ax.set_yticks([0.25, 0.75, 0.9], minor=True)\n","# ax.set_yticklabels([0.25, 0.75, 0.9], minor=True)\n","plt.grid(which='both', axis='y', alpha=0.7, zorder=1)\n","plt.ylabel('Accuracy')\n","plt.ylim([0, 1])\n","plt.title('Accuracy score of the selected binary classifiers')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fig_dir, f'eval_accuracy_nosyn_bin.pdf'))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4YL-AhHJZM7O"},"source":["### Confusion Matrixes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":26087,"status":"ok","timestamp":1664739356960,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"6oEWg1sjZMFt","outputId":"99aa864a-222a-48ca-bf91-87e4ca3e5584"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay\n","\n","for model_type in models.keys():\n","    model = models[model_type]['model']\n","    x_test = models[model_type]['Xtest']\n","    y_test = models[model_type]['Ytest']\n","    y_pred = model.predict(x_test)\n","    conf_mat = confusion_matrix(y_test, y_pred)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=model.classes_)\n","    # disp = ConfusionMatrixDisplay.from_estimator(model,\n","    #                                              x_test, y_test,\n","    #                                             #  display_labels=model.classes_,\n","    #                                              cmap=plt.cm.Blues)\n","    # disp.ax_.set_title(f'{model_type} Confusion Matrix')\n","    disp.plot(cmap=plt.cm.Blues)\n","    plt.title(f'{clean_name(model_type)} Confusion Matrix')\n","    plt.savefig(os.path.join(fig_dir, f'conf_mat_{model_type}.pdf'))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3651,"status":"ok","timestamp":1664737149585,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"uCapi4pq4IYa","outputId":"c355e5a7-62ce-488d-f3e6-94a934ad64ee"},"outputs":[],"source":["fig = plt.figure()\n","\n","c_matrixes = []\n","\n","for model_type in models.keys():\n","    model = models[model_type]['model']\n","    x_test = models[model_type]['Xtest']\n","    y_test = models[model_type]['Ytest']\n","\n","    if accuracy_scores[model_type] > 0.8:\n","        y_pred = model.predict(x_test)\n","        conf_mat = confusion_matrix(y_test, y_pred)\n","        disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=model.classes_)\n","        # disp = ConfusionMatrixDisplay.from_estimator(model,\n","        #                                              x_test, y_test,\n","        #                                             #  display_labels=model.classes_,\n","        #                                              cmap=plt.cm.Blues)\n","        # disp.ax_.set_title(f'{model_type} Confusion Matrix')\n","        c_matrixes.append(disp)\n","        # disp.plot(cmap=plt.cm.Blues)\n","\n","for i, cm in enumerate(c_matrixes):\n","    plt.subplot(len(c_matrixes), 2, i+1)\n","    disp.plot(cmap=plt.cm.Blues)\n","plt.title(f'Best Confusion Matrix')\n","plt.savefig(os.path.join(fig_dir, f'conf_mat_best.pdf'))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"be9Mr5JXQbcs"},"source":["## Feature importance"]},{"cell_type":"markdown","metadata":{"id":"tul59DdxUbZR"},"source":["Let us extract the most important features from the RFC and the Vectorizer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":771},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1664737149871,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"BJPTLwKI_WTn","outputId":"678758ba-3493-4deb-e8b5-fa16da8af915"},"outputs":[],"source":["pipeline = models['DecisionTree_8020']['model'].best_estimator_\n","vectorizer = pipeline.steps[0][1]\n","decision_tree = pipeline.steps[1][1]\n","# Get indeces of sorted importance values, then the sorted feature names\n","sorted_idx = (-decision_tree.feature_importances_).argsort()\n","features_names = vectorizer.get_feature_names_out()[sorted_idx]\n","features_vals = decision_tree.feature_importances_[sorted_idx]\n","# Logging\n","table_data = []\n","for elem in zip(features_names, features_vals):\n","    table_data.append(elem)\n","feature_importance = pd.DataFrame(table_data, columns=['Feature Name', 'Importance'])\n","feature_importance"]},{"cell_type":"markdown","metadata":{"id":"JyQgYTmRUm_d"},"source":["Sklearn default way of computing the feature importance is to measure, at training time, how well each feature decreases the impurity of the split. The measurement is then averaged across the training steps and averaged again across the ensembles, such that a single importance score per feature is obtained [1].\n","\n","Another possible method for estimating features importance is to utilize _permutations of features_ [2]: intuitively, given a fitted model with a known performance score, each feature, _i.e._ column, is randomly swapped and the score of the classifier is computed. The average drop in perfomance can then be used as a score for feature importance. In fact, the idea is that features with the highest performance drop will account the most and therefore have the highest importance score.\n","\n","References:\n","\n","* [1] Random Forest Feature Importance Computed in 3 Ways with Python, link: https://mljar.com/blog/feature-importance-in-random-forest/\n","* [2] Permutation feature importance, link: https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance"]},{"cell_type":"markdown","metadata":{"id":"G2epzayiDrGe"},"source":["### Plotting all Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":25516,"status":"ok","timestamp":1664737175377,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"Hi8S8A-aDq0u","outputId":"43320703-be56-4566-db1e-5416d8477859"},"outputs":[],"source":["for model_type in models.keys():\n","    if 'DecisionTree' in model_type or 'RandomForest' in model_type or 'GradientBoosting' in model_type:\n","        pipeline = models[model_type]['model'].best_estimator_\n","        vectorizer = pipeline.steps[0][1]\n","        decision_tree = pipeline.steps[1][1]\n","        # Get indeces of sorted importance values, then the sorted feature names\n","        sorted_idx = (-decision_tree.feature_importances_).argsort()\n","        features_names = vectorizer.get_feature_names_out()[sorted_idx]\n","        features_vals = decision_tree.feature_importances_[sorted_idx]\n","        # Logging\n","        table_data = []\n","        for elem in zip(features_names, features_vals):\n","            table_data.append(elem)\n","        feature_importance = pd.DataFrame(table_data, columns=['Feature Name', 'Importance'])\n","        # Plotting\n","        ax = sns.barplot(x='Importance', y='Feature Name', data=feature_importance, orient='h', color='C0')\n","        plt.title(f'Feature Importance for {clean_name(model_type)}')\n","        plt.grid('both', axis='x', alpha=0.7)\n","        plt.savefig(os.path.join(fig_dir, f'feature_importance_{model_type}.pdf'), bbox_inches='tight')\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_L2bhajVgmJd"},"source":["### SHAP Explainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5369,"status":"ok","timestamp":1664459437254,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"0k1CvLxPg1Od","outputId":"7171063d-bfac-44a4-89c6-f0f97d0646c5"},"outputs":[],"source":["# !pip install shap"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"executionInfo":{"elapsed":244,"status":"error","timestamp":1664460836117,"user":{"displayName":"Stefano Ribes","userId":"00904170517311730721"},"user_tz":-120},"id":"2H_TC4Z6goDg","outputId":"975048c2-86ef-4142-b8b5-cba6727f826e"},"outputs":[],"source":["# import shap\n","\n","# for model_type in models.keys():\n","#     if 'DecisionTree' in model_type or 'RandomForest' in model_type or 'GradientBoosting' in model_type:\n","#         pipeline = models[model_type]['model'].best_estimator_\n","#         vectorizer = pipeline.steps[0][1]\n","#         classifier = pipeline.steps[1][1]\n","\n","#         X = vectorizer.fit_transform(models[model_type]['Xtest'])\n","#         explainer = shap.TreeExplainer(classifier, X) #, feature_names=performance_features + synthesis_features)\n","#         print(explainer)\n","#         shap_values = explainer(X)\n","#         # print(shap_values.data)\n","#         print(shap_values[0].shape)\n","\n","\n","#         idx = 3\n","#         exp = shap.Explanation(shap_values.values,\n","#                                shap_values.base_values[0][0],\n","#                                data=shap_values.data,\n","#                             #    feature_names=performance_features + synthesis_features\n","#                                )\n","#         print(exp[idx].shape)\n","#         shap.plots.waterfall(exp[idx])\n","\n","#         # shap.force_plot(explainer.expected_value, shap_values, X)\n","#         # shap.summary_plot(shape_values, X)\n","\n","#         # Visualize the first prediction's explanation\n","#         # shap.plots.waterfall(shap_values)\n","#         plt.show()\n","#         break"]},{"cell_type":"markdown","metadata":{"id":"AoTZPs6oBVNG"},"source":["## Deep Learning (More training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhqWUlrokWTj"},"outputs":[],"source":["from __future__ import print_function\n","import tensorflow as tf\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","from keras import Input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OrrRbKWlyBc"},"outputs":[],"source":["# x_train = datasets['_6535_nosyn']['Xtrain']\n","# y_train = datasets['_6535_nosyn']['Ytrain']\n","# x_test = datasets['_6535_nosyn']['Xtest']\n","# y_test = datasets['_6535_nosyn']['Ytest']\n","\n","# df = {'Train': {}, 'Test': {}}\n","\n","# unique, train_counts = np.unique(y_train, return_counts=True)\n","# unique, test_counts = np.unique(y_test, return_counts=True)\n","# df['Train'] = train_counts\n","# df['Test'] = test_counts\n","# df = pd.DataFrame(df)\n","# df\n","\n","# for c in hth_classes:\n","#     df['Train'][c] = np.count(y_train.astype(np.int32)\n","# df = pd.DataFrame({'Train': y_train.astype(np.int32),\n","#                    'Test': y_train.astype(np.int32)\n","#                    })\n","# df\n","\n","# bar1 = sns.barplot(x='Train',  y=df['Train'], data=df, color='darkblue')\n","# bar2 = sns.barplot(x='Test', y=df['Test'], data=df, color='lightblue')\n","\n","# plt.legend()\n","# plt.grid('both')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8Dny5uTBakl"},"outputs":[],"source":["# x_train = datasets['_6535_nosyn']['Xtrain'].to_dict('records')\n","# y_train = datasets['_6535_nosyn']['Ytrain']\n","# x_test = datasets['_6535_nosyn']['Xtest'].to_dict('records')\n","# y_test = datasets['_6535_nosyn']['Ytest']\n","\n","df = get_dataset(triggered, not_triggered)\n","# df = get_dataset(triggered, not_triggered).drop([x for x in synthesis_features if x not in ['power', 'Temp']], axis=1)\n","# df = get_dataset(triggered, not_triggered).drop(synthesis_features, axis=1)\n","x_train, y_train, x_test, y_test = split_dataset(df, train_perc=0.80)\n","\n","x_train = x_train.to_dict('records')\n","x_test = x_test.to_dict('records')\n","y_train = preprocess_labels(y_train)\n","y_test = preprocess_labels(y_test)\n","\n","unique, train_counts = np.unique(y_train, return_counts=True)\n","unique, test_counts = np.unique(y_test, return_counts=True)\n","print(pd.DataFrame({'Train': train_counts, 'Test': test_counts}))\n","print('')\n","\n","# Vectorize features\n","vect = DictVectorizer(sparse=False)\n","vect.fit(x_train)\n","x_train = vect.fit_transform(x_train)\n","x_test = vect.fit_transform(x_test)\n","# Standardize Features\n","norm = StandardScaler()\n","norm.fit(x_train)\n","x_train = norm.fit_transform(x_train)\n","x_test = norm.fit_transform(x_test)\n","# Convert labels strings to integers\n","for k in hth_dict.keys():\n","    y_train[y_train == hth_dict[k]] = int(k)\n","    y_test[y_test == hth_dict[k]] = int(k)\n","y_train = y_train.astype(np.int32)\n","y_test = y_test.astype(np.int32)\n","# Convert to one-hot encoding\n","onehot_enc = keras.layers.CategoryEncoding(num_tokens=len(hth_classes), output_mode='one_hot')\n","y_test = onehot_enc(y_test)\n","y_train = onehot_enc(y_train)\n","print(x_train.shape)\n","print(x_test.shape)\n","print(f'train mean: {np.mean(x_train):.4f}')\n","print(f'train std: {np.std(x_train):.4f}')\n","print(f'test mean: {np.mean(x_test):.4f}')\n","print(f'test std: {np.std(x_test):.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnooGQEB1a80"},"outputs":[],"source":["history = None\n","history_prev = None\n","num_epochs_prev = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kc2-HwATDZ9o"},"outputs":[],"source":["l2 = tf.keras.regularizers.l2(0.001)\n","num_epochs = 200\n","batch_size = 32\n","loss = 'categorical_crossentropy'\n","# opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.8, beta_2=0.99)\n","opt = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9)\n","# opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, momentum=0.9)\n","\n","deep_model = keras.Sequential([\n","    keras.layers.Input(shape=(x_train.shape[-1])),\n","    keras.layers.Dense(32, activation='relu'), #, kernel_regularizer=l2),\n","    keras.layers.BatchNormalization(),\n","    # keras.layers.Dropout(0.5),\n","    keras.layers.Dense(32, activation='relu'), #, kernel_regularizer=l2),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Dense(32, activation='relu'), #, kernel_regularizer=l2),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Dense(len(hth_classes), activation='softmax'),\n","])\n","\n","class HTHclassifier(tf.keras.Model):\n","  def __init__(self):\n","    super().__init__()\n","    self.dense1 = keras.layers.Dense(4, activation='relu')\n","    self.dense2 = keras.layers.Dense(5, activation='softmax')\n","    self.detector = keras.Sequential([\n","        keras.layers.Input(shape=(x_train.shape[-1])),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.BatchNormalization(),\n","        # keras.layers.Dropout(0.5),\n","        # keras.layers.Dense(128, activation='relu'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Dense(1, activation='sigmoid'),\n","    ])\n","    self.classifier = keras.Sequential([\n","        keras.layers.Input(shape=(x_train.shape[-1])),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.BatchNormalization(),\n","        # keras.layers.Dropout(0.5),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Dense(128, activation='relu'),\n","        keras.layers.BatchNormalization(),\n","        keras.layers.Dense(len(hth_classes) - 1, activation='softmax'),\n","    ])\n","\n","  def call(self, inputs):\n","    is_trojan = self.detector(inputs)\n","    hth_class = self.classifier(inputs)\n","    return self.dense2(x)\n","\n","\n","deep_model.compile(loss=loss, optimizer=opt, metrics='accuracy')\n","deep_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7vVvXe5Pz1k"},"outputs":[],"source":["early_stop_loss = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n","early_stop_val_acc = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max', min_delta=0.05, verbose=1)\n","callbacks = None # [early_stop_val_acc]\n","\n","history = deep_model.fit(x_train, y_train,\n","               epochs=num_epochs,\n","               batch_size=batch_size,\n","               validation_data=(x_test, y_test),\n","               callbacks=callbacks,\n","               verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXiZrQVzRipO"},"outputs":[],"source":["for metric in history.history.keys():\n","    if 'loss' in metric:\n","        x = range(len(history.history['loss']))\n","        y = history.history[metric]\n","        plt.plot(x, y, label=metric)\n","        if history_prev is not None:\n","            x = range(len(history_prev.history['loss']))\n","            y = history_prev.history[metric]\n","            plt.plot(x, y, '--', label=f'Previous {metric}', alpha=0.6)\n","plt.grid('both')\n","plt.legend()\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.show()\n","\n","for metric in history.history.keys():\n","    if 'accuracy' in metric:\n","        x = range(len(history.history['accuracy']))\n","        y = history.history[metric]\n","        plt.plot(x, y, label=metric)\n","        if history_prev is not None:\n","            x = range(len(history_prev.history['accuracy']))\n","            y = history_prev.history[metric]\n","            plt.plot(x, y, '--', label=f'Previous {metric}', alpha=0.6)\n","plt.grid('both')\n","plt.legend()\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epochs')\n","plt.show()\n","\n","history_prev = history\n","num_epochs_prev = num_epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGeqvqGuQDRo"},"outputs":[],"source":["l, a = deep_model.evaluate(x_train, y_train, verbose=0)\n","print(f'Train accuracy: {a * 100:.2f}%')\n","print(f'Train loss:     {l:.4f}')\n","\n","l, a = deep_model.evaluate(x_test, y_test, verbose=0)\n","print(f'Evaluation accuracy: {a * 100:.2f}%')\n","print(f'Evaluation loss:     {l:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"N1ri3liY_fnf"},"source":["### Self-Supervised Learning (TabNet)\n","\n","[TabNet Colab example.](https://colab.research.google.com/drive/1P8Obe07DP3VeOld08ThyT1HnChLip_LO)"]},{"cell_type":"markdown","metadata":{"id":"pQI1nqgo39z7"},"source":["## Converting Notebook to PDF\n","\n","The following two cells can be ignored for grading, as they just convert this notebook into a PDF file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PXT059d30qG"},"outputs":[],"source":["%%capture\n","!apt-get update\n","!apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic\n","!apt-get install -y inkscape\n","!add-apt-repository -y universe\n","!add-apt-repository -y ppa:inkscape.dev/stable\n","!apt-get update -y\n","!apt install -y inkscape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVgdQGHAtQJm"},"outputs":[],"source":["%%capture\n","import re\n","\n","ASSIGNMENT_NAME = 'HTH_Detection'\n","pdf_dir = os.path.join(os.path.abspath(''), 'drive', 'MyDrive')\n","pdf_dir = os.path.join(pdf_dir, 'Colab Notebooks', 'hth_detection')\n","pdf_filename = re.escape(os.path.join(pdf_dir, ASSIGNMENT_NAME)) + '.ipynb'\n","\n","!jupyter nbconvert --to pdf --TemplateExporter.exclude_input=False $pdf_filename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjLG-u2G0aFM"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMJeSxYkFc+CQ//Lp1Ul14y","collapsed_sections":[],"mount_file_id":"1sMNNrs7crcaSTpWAKSQG6BNoK8-KPbMg","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
